# GitHub Search Limitations - Hybrid Search Strategy

**Issue**: [#62 - Does not discover all dandisets: might need explicit listing/traversal of 'busy' organizations](https://github.com/datalad/datalad-usage-dashboard/issues/62)

## Problem Analysis

### Current Limitations
- GitHub search API limited to 1000 results total (100 per page × 10 pages max)
- Global searches miss repositories in high-activity organizations
- Cannot effectively exclude organizations from global searches due to query length limits

### Current Search Approach
```python
# Limited global searches
search("code", "path:.datalad filename:config fork:true")
search("commits", '"DATALAD RUNCMD" merge:false is:public')
```

### Validation Results
Organization-specific searches reveal significant gaps:

| Organization | Currently Indexed | GitHub Search Results | Gap |
|--------------|-------------------|----------------------|-----|
| dandisets | 225 | 0 (.datalad files) | N/A (RUNCMD only) |
| OpenNeuroDatasets | 757 | 780+ | ~23+ missing |
| datasets-mila | 320 | 5 | Over-indexed via RUNCMD |
| OpenNeuroDerivatives | 290 | 556+ | **~266+ missing** |

## Proposed Solution: Hybrid Search Strategy

### Architecture Overview
1. **Global search with exclusions** → discovers new organizations  
2. **Targeted organization searches** → comprehensive coverage of busy orgs

### Benefits
- Bypasses 1000-result global limit via organization-specific searches
- Maintains discovery of new organizations through filtered global search
- Scalable approach that adapts as data grows

## Implementation Plan

### Phase 1: Organization Analysis & Exclusion Logic

**1.1 Create exclusion criteria:**
```python
def get_organizations_for_exclusion(current_repos, threshold=30):
    """Get orgs with >threshold repos to exclude from global search"""
    org_counts = Counter()
    for repo in current_repos:
        org = repo['name'].split('/')[0] if isinstance(repo, dict) else repo.name.split('/')[0]
        org_counts[org] += 1
    return [org for org, count in org_counts.items() if count >= threshold]
```

**1.2 Build exclusion query string:**
```python  
def build_exclusion_query(orgs, max_length=1000):
    """Build -org:name1 -org:name2... string within GitHub limits"""
    exclusions = []
    query_length = 0
    
    # Sort by repo count (descending) to prioritize excluding busiest orgs
    for org in sorted(orgs, key=lambda x: org_counts[x], reverse=True):
        addition = f" -org:{org}"
        if query_length + len(addition) > max_length:
            break
        exclusions.append(addition)
        query_length += len(addition)
    
    return "".join(exclusions)
```

### Phase 2: Enhanced GitHubSearcher Methods

**2.1 Add new search methods:**
```python
def search_dataset_repos_with_exclusions(self, exclusions: str) -> Iterator[SearchHit]:
    """Global search excluding busy organizations"""
    query = f"path:.datalad filename:config fork:true{exclusions}"
    log.info(f"Global search with exclusions: {query}")
    for hit in self.search("code", query):
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found %s", repo.name)
        yield repo

def search_runcmds_with_exclusions(self, exclusions: str) -> Iterator[tuple[SearchHit, bool]]:
    """Global RUNCMD search excluding busy organizations"""
    query = f'"DATALAD RUNCMD" merge:false is:public{exclusions}'
    log.info(f"Global RUNCMD search with exclusions: {query}")
    for hit in self.search("commits", query):
        container_run = is_container_run(hit["commit"]["message"])
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found commit %s in %s (container run: %s)", 
                hit["sha"][:7], repo.name, container_run)
        yield (repo, container_run)

def search_dataset_repos_in_org(self, org: str) -> Iterator[SearchHit]:  
    """Search for datasets within specific organization"""
    query = f"org:{org} path:.datalad filename:config"
    log.info(f"Searching for .datalad datasets in {org}")
    for hit in self.search("code", query):
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found %s", repo.name)
        yield repo

def search_runcmds_in_org(self, org: str) -> Iterator[tuple[SearchHit, bool]]:
    """Search for DATALAD RUNCMD within specific organization"""
    query = f'org:{org} "DATALAD RUNCMD" merge:false'
    log.info(f"Searching for RUNCMD commits in {org}")
    for hit in self.search("commits", query):
        container_run = is_container_run(hit["commit"]["message"])
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found commit %s in %s (container run: %s)",
                hit["sha"][:7], repo.name, container_run)
        yield (repo, container_run)
```

**2.2 Replace existing get_datalad_repos method:**
```python
def get_datalad_repos(self, excluded_orgs: list[str] = None) -> list[SearchResult]:
    """
    Hybrid search: global search with exclusions + targeted org searches
    
    Args:
        excluded_orgs: Organizations to exclude from global search and search individually
    """
    excluded_orgs = excluded_orgs or []
    exclusions = build_exclusion_query(excluded_orgs) if excluded_orgs else ""
    
    log.info(f"Using hybrid search strategy with {len(excluded_orgs)} excluded orgs")
    
    # Phase 1: Global search excluding busy orgs
    log.info("Phase 1: Global searches with exclusions")
    datasets = set(self.search_dataset_repos_with_exclusions(exclusions))
    runcmds = {}
    
    for repo, container_run in self.search_runcmds_with_exclusions(exclusions):
        runcmds[repo] = container_run or runcmds.get(repo, False)
    
    log.info(f"Global search found: {len(datasets)} datasets, {len(runcmds)} runcmd repos")
    
    # Phase 2: Organization-specific searches
    log.info("Phase 2: Organization-specific searches")
    for org in excluded_orgs:
        log.info(f"Searching within organization: {org}")
        
        # Add datasets from this org
        org_datasets = set(self.search_dataset_repos_in_org(org))
        datasets.update(org_datasets)
        
        # Add runcmds from this org  
        for repo, container_run in self.search_runcmds_in_org(org):
            runcmds[repo] = container_run or runcmds.get(repo, False)
        
        log.info(f"Organization {org} added: {len(org_datasets)} datasets")
    
    log.info(f"Total after hybrid search: {len(datasets)} datasets, {len(runcmds)} runcmd repos")
    
    # Convert to SearchResult objects
    results = []
    for repo in datasets | runcmds.keys():
        results.append(SearchResult(
            id=repo.id, url=repo.url, name=repo.name,
            dataset=repo in datasets,
            run=repo in runcmds, 
            container_run=runcmds.get(repo, False),
        ))
    
    return sorted(results, key=attrgetter("name"))
```

### Phase 3: Configuration & Integration

**3.1 Add configuration options:**
```python
# In config.py
EXCLUSION_THRESHOLD = 30  # Orgs with >30 repos get excluded from global search
MAX_EXCLUSION_QUERY_LENGTH = 1000  # GitHub query length limit
```

**3.2 Update GitHubUpdater integration:**
```python  
class GitHubUpdater(BaseModel, Updater[GitHubRepo, SearchResult, GitHubSearcher]):
    # ... existing fields ...
    excluded_orgs: list[str] = Field(default_factory=list)
    
    @classmethod
    def from_collection(cls, host: RepoHost, collection: list[GitHubRepo]) -> GitHubUpdater:
        all_repos: dict[int, GitHubRepo] = {}
        noid_repos: list[GitHubRepo] = []
        for repo in collection:
            if repo.id is not None:
                all_repos[repo.id] = repo
            else:
                noid_repos.append(repo)
        
        # Calculate organizations to exclude
        excluded_orgs = get_organizations_for_exclusion(
            current_repos=collection,
            threshold=EXCLUSION_THRESHOLD
        )
        
        return cls(
            all_repos=all_repos, 
            noid_repos=noid_repos,
            excluded_orgs=excluded_orgs
        )

    def get_searcher(self, **kwargs: Any) -> GitHubSearcher:
        searcher = GitHubSearcher(**kwargs)
        # Pass excluded orgs to searcher via get_datalad_repos call
        return searcher

    def register_repo(self, sr: SearchResult, searcher: GitHubSearcher) -> None:
        # ... existing implementation unchanged ...
        pass

    def get_new_collection(self, searcher: GitHubSearcher) -> list[GitHubRepo]:
        # Use hybrid search strategy
        search_results = searcher.get_datalad_repos(excluded_orgs=self.excluded_orgs)
        
        # Process search results
        for sr in search_results:
            self.register_repo(sr, searcher)
        
        # ... rest of existing implementation ...
        return collection
```

### Phase 4: Testing & Validation

**4.1 Create test script:**
```python
#!/usr/bin/env python3
"""Test hybrid search strategy effectiveness"""

def test_hybrid_search():
    """Compare current vs hybrid search results"""
    from src.find_datalad_repos.github import GitHubSearcher
    from src.find_datalad_repos.config import EXCLUSION_THRESHOLD
    import os
    import json
    
    # Load current data
    with open('datalad-repos.json') as f:
        data = json.load(f)
    
    # Test with high-activity organizations
    test_orgs = ['OpenNeuroDerivatives', 'OpenNeuroDatasets', 'datasets-mila']
    
    searcher = GitHubSearcher(token=os.environ['GITHUB_TOKEN'])
    
    print("=== Testing Hybrid Search Strategy ===")
    
    # Current indexed counts
    current_counts = {}
    for repo in data['github']:
        org = repo['name'].split('/')[0]
        current_counts[org] = current_counts.get(org, 0) + 1
    
    for org in test_orgs:
        print(f"\n--- {org} ---")
        print(f"Currently indexed: {current_counts.get(org, 0)} repositories")
        
        # Test organization-specific search
        datasets = list(searcher.search_dataset_repos_in_org(org))
        runcmds = list(searcher.search_runcmds_in_org(org))
        
        print(f"Org search found: {len(datasets)} datasets, {len(runcmds)} runcmd repos")
        
        # Calculate unique repositories
        all_repos = set(r.name for r in datasets)
        all_repos.update(r[0].name for r in runcmds)
        
        print(f"Total unique repos discoverable: {len(all_repos)}")
        print(f"Potential new discoveries: {len(all_repos) - current_counts.get(org, 0)}")

if __name__ == '__main__':
    test_hybrid_search()
```

**4.2 Gradual rollout plan:**
1. **Test phase**: Run test script to validate approach
2. **Pilot phase**: Deploy with 2-3 organizations (OpenNeuroDerivatives, OpenNeuroDatasets)
3. **Validation phase**: Verify new discoveries are valid DataLad repositories
4. **Full deployment**: Apply to all organizations above threshold
5. **Monitoring**: Track discovery improvements and API usage

## Expected Outcomes

### Quantitative Improvements
Based on validation testing:
- **OpenNeuroDerivatives**: +266 repositories (from 290 to 556+)
- **OpenNeuroDatasets**: +23 repositories (from 757 to 780+)
- **Total estimated improvement**: 300+ additional repositories discovered

### Operational Benefits
- **Comprehensive coverage**: No longer limited by 1000-result global search cap
- **Scalable approach**: Automatically adapts as organizations grow
- **Discovery preservation**: Global search still finds new organizations
- **Backward compatible**: No changes to data structures or external interfaces

## Implementation Notes

### API Rate Limiting
- Organization searches will increase API usage
- Current rate limiting (10s between searches, 45s after abuse detection) should be sufficient
- Monitor for need to adjust delays

### Query Length Management
- GitHub search queries have practical limits (~1000 characters)
- Exclusion list prioritizes busiest organizations first
- Fallback: if exclusion list gets too long, increase threshold

### Error Handling
- Organization searches may hit different rate limits than global searches
- Need robust error handling for partial failures
- Consider implementing retry logic for individual organization searches

## Future Enhancements

1. **Dynamic thresholds**: Adjust exclusion threshold based on total repository count
2. **Parallel organization searches**: Search multiple orgs concurrently (within rate limits)
3. **Contents API validation**: Use `/repos/owner/repo/contents/.datalad/config` to validate discoveries
4. **Incremental updates**: Track per-organization last-updated timestamps for more efficient re-runs