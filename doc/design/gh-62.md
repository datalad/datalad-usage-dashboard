# GitHub Search Limitations - Hybrid Search Strategy

**Issue**: [#62 - Does not discover all dandisets: might need explicit listing/traversal of 'busy' organizations](https://github.com/datalad/datalad-usage-dashboard/issues/62)

## Problem Analysis

### Current Limitations
- GitHub search API limited to 1000 results total (100 per page × 10 pages max)
- Global searches miss repositories in high-activity organizations
- Cannot effectively exclude organizations from global searches due to query length limits

### Current Search Approach
```python
# Limited global searches
search("code", "path:.datalad filename:config fork:true")
search("commits", '"DATALAD RUNCMD" merge:false is:public')
```

### Validation Results
Organization-specific searches reveal significant gaps:

| Organization | Currently Indexed | GitHub Search Results | Gap |
|--------------|-------------------|----------------------|-----|
| dandisets | 225 | 0 (.datalad files) | N/A (RUNCMD only) |
| OpenNeuroDatasets | 757 | 780+ | ~23+ missing |
| datasets-mila | 320 | 5 | Over-indexed via RUNCMD |
| OpenNeuroDerivatives | 290 | 556+ | **~266+ missing** |

## Proposed Solution: Hybrid Search Strategy

### Architecture Overview
1. **Global search with exclusions** → discovers new organizations
2. **Targeted organization searches** → comprehensive coverage of busy orgs

### Benefits
- Bypasses 1000-result global limit via organization-specific searches
- Maintains discovery of new organizations through filtered global search
- Scalable approach that adapts as data grows

## Implementation Plan

### Phase 1: Organization Analysis & Exclusion Logic

**1.1 Create exclusion criteria:**
```python
def get_organizations_for_exclusion(current_repos, threshold=30):
    """Get orgs with >threshold repos to exclude from global search"""
    org_counts = Counter()
    for repo in current_repos:
        org = repo['name'].split('/')[0] if isinstance(repo, dict) else repo.name.split('/')[0]
        org_counts[org] += 1
    return [org for org, count in org_counts.items() if count >= threshold]
```

**1.2 Build exclusion query string:**
```python
def build_exclusion_query(orgs, max_length=1000):
    """Build -org:name1 -org:name2... string within GitHub limits"""
    exclusions = []
    query_length = 0

    # Sort by repo count (descending) to prioritize excluding busiest orgs
    for org in sorted(orgs, key=lambda x: org_counts[x], reverse=True):
        addition = f" -org:{org}"
        if query_length + len(addition) > max_length:
            break
        exclusions.append(addition)
        query_length += len(addition)

    return "".join(exclusions)
```

### Phase 2: Enhanced GitHubSearcher Methods

**2.1 Add new search methods:**
```python
def search_dataset_repos_with_exclusions(self, exclusions: str) -> Iterator[SearchHit]:
    """Global search excluding busy organizations"""
    query = f"path:.datalad filename:config fork:true{exclusions}"
    log.info(f"Global search with exclusions: {query}")
    for hit in self.search("code", query):
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found %s", repo.name)
        yield repo

def search_runcmds_with_exclusions(self, exclusions: str) -> Iterator[tuple[SearchHit, bool]]:
    """Global RUNCMD search excluding busy organizations"""
    query = f'"DATALAD RUNCMD" merge:false is:public{exclusions}'
    log.info(f"Global RUNCMD search with exclusions: {query}")
    for hit in self.search("commits", query):
        container_run = is_container_run(hit["commit"]["message"])
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found commit %s in %s (container run: %s)",
                hit["sha"][:7], repo.name, container_run)
        yield (repo, container_run)

def search_dataset_repos_in_org(self, org: str) -> Iterator[SearchHit]:
    """Search for datasets within specific organization"""
    query = f"org:{org} path:.datalad filename:config"
    log.info(f"Searching for .datalad datasets in {org}")
    for hit in self.search("code", query):
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found %s", repo.name)
        yield repo

def search_runcmds_in_org(self, org: str) -> Iterator[tuple[SearchHit, bool]]:
    """Search for DATALAD RUNCMD within specific organization"""
    query = f'org:{org} "DATALAD RUNCMD" merge:false'
    log.info(f"Searching for RUNCMD commits in {org}")
    for hit in self.search("commits", query):
        container_run = is_container_run(hit["commit"]["message"])
        repo = SearchHit.from_repository(hit["repository"])
        log.info("Found commit %s in %s (container run: %s)",
                hit["sha"][:7], repo.name, container_run)
        yield (repo, container_run)
```

**2.2 Replace existing get_datalad_repos method:**
```python
def get_datalad_repos(self, excluded_orgs: list[str] = None) -> list[SearchResult]:
    """
    Hybrid search: global search with exclusions + targeted org searches

    Args:
        excluded_orgs: Organizations to exclude from global search and search individually
    """
    excluded_orgs = excluded_orgs or []
    exclusions = build_exclusion_query(excluded_orgs) if excluded_orgs else ""

    log.info(f"Using hybrid search strategy with {len(excluded_orgs)} excluded orgs")

    # Phase 1: Global search excluding busy orgs
    log.info("Phase 1: Global searches with exclusions")
    datasets = set(self.search_dataset_repos_with_exclusions(exclusions))
    runcmds = {}

    for repo, container_run in self.search_runcmds_with_exclusions(exclusions):
        runcmds[repo] = container_run or runcmds.get(repo, False)

    log.info(f"Global search found: {len(datasets)} datasets, {len(runcmds)} runcmd repos")

    # Phase 2: Organization-specific searches
    log.info("Phase 2: Organization-specific searches")
    for org in excluded_orgs:
        log.info(f"Searching within organization: {org}")

        # Add datasets from this org
        org_datasets = set(self.search_dataset_repos_in_org(org))
        datasets.update(org_datasets)

        # Add runcmds from this org
        for repo, container_run in self.search_runcmds_in_org(org):
            runcmds[repo] = container_run or runcmds.get(repo, False)

        log.info(f"Organization {org} added: {len(org_datasets)} datasets")

    log.info(f"Total after hybrid search: {len(datasets)} datasets, {len(runcmds)} runcmd repos")

    # Convert to SearchResult objects
    results = []
    for repo in datasets | runcmds.keys():
        results.append(SearchResult(
            id=repo.id, url=repo.url, name=repo.name,
            dataset=repo in datasets,
            run=repo in runcmds,
            container_run=runcmds.get(repo, False),
        ))

    return sorted(results, key=attrgetter("name"))
```

### Phase 3: Configuration & Integration

**3.1 Add configuration options:**
```python
# In config.py
EXCLUSION_THRESHOLD = 30  # Orgs with >30 repos get excluded from global search
MAX_EXCLUSION_QUERY_LENGTH = 1000  # GitHub query length limit
```

**3.2 Update GitHubUpdater integration:**
```python
class GitHubUpdater(BaseModel, Updater[GitHubRepo, SearchResult, GitHubSearcher]):
    # ... existing fields ...
    excluded_orgs: list[str] = Field(default_factory=list)

    @classmethod
    def from_collection(cls, host: RepoHost, collection: list[GitHubRepo]) -> GitHubUpdater:
        all_repos: dict[int, GitHubRepo] = {}
        noid_repos: list[GitHubRepo] = []
        for repo in collection:
            if repo.id is not None:
                all_repos[repo.id] = repo
            else:
                noid_repos.append(repo)

        # Calculate organizations to exclude
        excluded_orgs = get_organizations_for_exclusion(
            current_repos=collection,
            threshold=EXCLUSION_THRESHOLD
        )

        return cls(
            all_repos=all_repos,
            noid_repos=noid_repos,
            excluded_orgs=excluded_orgs
        )

    def get_searcher(self, **kwargs: Any) -> GitHubSearcher:
        searcher = GitHubSearcher(**kwargs)
        # Pass excluded orgs to searcher via get_datalad_repos call
        return searcher

    def register_repo(self, sr: SearchResult, searcher: GitHubSearcher) -> None:
        # ... existing implementation unchanged ...
        pass

    def get_new_collection(self, searcher: GitHubSearcher) -> list[GitHubRepo]:
        # Use hybrid search strategy
        search_results = searcher.get_datalad_repos(excluded_orgs=self.excluded_orgs)

        # Process search results
        for sr in search_results:
            self.register_repo(sr, searcher)

        # ... rest of existing implementation ...
        return collection
```

### Phase 4: Testing & Validation

**4.1 Create test script:**
```python
#!/usr/bin/env python3
"""Test hybrid search strategy effectiveness"""

def test_hybrid_search():
    """Compare current vs hybrid search results"""
    from src.find_datalad_repos.github import GitHubSearcher
    from src.find_datalad_repos.config import EXCLUSION_THRESHOLD
    import os
    import json

    # Load current data
    with open('datalad-repos.json') as f:
        data = json.load(f)

    # Test with high-activity organizations
    test_orgs = ['OpenNeuroDerivatives', 'OpenNeuroDatasets', 'datasets-mila']

    searcher = GitHubSearcher(token=os.environ['GITHUB_TOKEN'])

    print("=== Testing Hybrid Search Strategy ===")

    # Current indexed counts
    current_counts = {}
    for repo in data['github']:
        org = repo['name'].split('/')[0]
        current_counts[org] = current_counts.get(org, 0) + 1

    for org in test_orgs:
        print(f"\n--- {org} ---")
        print(f"Currently indexed: {current_counts.get(org, 0)} repositories")

        # Test organization-specific search
        datasets = list(searcher.search_dataset_repos_in_org(org))
        runcmds = list(searcher.search_runcmds_in_org(org))

        print(f"Org search found: {len(datasets)} datasets, {len(runcmds)} runcmd repos")

        # Calculate unique repositories
        all_repos = set(r.name for r in datasets)
        all_repos.update(r[0].name for r in runcmds)

        print(f"Total unique repos discoverable: {len(all_repos)}")
        print(f"Potential new discoveries: {len(all_repos) - current_counts.get(org, 0)}")

if __name__ == '__main__':
    test_hybrid_search()
```

**4.2 Gradual rollout plan:**
1. **Test phase**: Run test script to validate approach
2. **Pilot phase**: Deploy with 2-3 organizations (OpenNeuroDerivatives, OpenNeuroDatasets)
3. **Validation phase**: Verify new discoveries are valid DataLad repositories
4. **Full deployment**: Apply to all organizations above threshold
5. **Monitoring**: Track discovery improvements and API usage

## Expected Outcomes

### Quantitative Improvements
Based on validation testing:
- **OpenNeuroDerivatives**: +266 repositories (from 290 to 556+)
- **OpenNeuroDatasets**: +23 repositories (from 757 to 780+)
- **Total estimated improvement**: 300+ additional repositories discovered

### Operational Benefits
- **Comprehensive coverage**: No longer limited by 1000-result global search cap
- **Scalable approach**: Automatically adapts as organizations grow
- **Discovery preservation**: Global search still finds new organizations
- **Backward compatible**: No changes to data structures or external interfaces

## Implementation Notes

### API Rate Limiting
- Organization searches will increase API usage
- Current rate limiting (10s between searches, 45s after abuse detection) should be sufficient
- Monitor for need to adjust delays

### Query Length Management
- GitHub search queries have practical limits (~1000 characters)
- Exclusion list prioritizes busiest organizations first
- Fallback: if exclusion list gets too long, increase threshold

### Error Handling
- Organization searches may hit different rate limits than global searches
- Need robust error handling for partial failures
- Consider implementing retry logic for individual organization searches

## Critical Issue: Empty Search Results for Some Organizations

### Problem Discovery
GitHub search API returns empty results for certain organizations (e.g., "dandisets") even when:
- Organization has 200+ active repositories
- Repositories are public and accessible
- Repositories contain `.datalad/config` files

**Example**: `org:dandisets` search returns 0 results despite 217 active repositories with DataLad configs

### Root Cause
GitHub's code search has undocumented limitations/restrictions for certain organizations, possibly due to:
- Repository content indexing delays or failures
- Organization-level search restrictions
- Search index corruption or limitations

## Enhanced Solution: Three-Tier Search Strategy with Repository Enumeration Fallback

### Tier 1: Global Search (Existing)
- Use code search API with organization exclusions
- Efficient for discovering new organizations and repositories

### Tier 2: Organization-Specific Search (Existing)
- Use code search API scoped to specific organizations
- Works for most organizations but fails for some (e.g., "dandisets")

### Tier 3: Repository Enumeration Fallback (New)
When organization-specific search returns empty results but we know active repositories exist:

1. **Detect Fallback Condition**:
   ```python
   def needs_enumeration_fallback(org: str, search_results: list, known_repos: list) -> bool:
       """Determine if we need to enumerate repositories directly"""
       active_known = [r for r in known_repos
                       if r['name'].startswith(f"{org}/") and r.get('status') != 'gone']
       return len(search_results) == 0 and len(active_known) > 0
   ```

2. **Enumerate Organization Repositories**:
   ```python
   def enumerate_org_repositories(self, org: str) -> Iterator[dict]:
       """List all repositories in an organization"""
       page = 1
       while True:
           repos = self.get(f"/orgs/{org}/repos",
                          params={"per_page": 100, "page": page})
           if not repos:
               break
           yield from repos
           page += 1
   ```

3. **Check for DataLad Configuration**:
   ```python
   def check_datalad_config(self, owner: str, repo: str, branch: str) -> bool:
       """Check if repository has .datalad/config file"""
       try:
           url = f"https://raw.githubusercontent.com/{owner}/{repo}/refs/heads/{branch}/.datalad/config"
           response = requests.head(url, timeout=5)
           return response.status_code == 200
       except:
           return False
   ```

4. **Process Unknown Repositories**:
   ```python
   def process_enumerated_repos(self, org: str, known_repos: set) -> Iterator[SearchResult]:
       """Process repos found via enumeration that aren't in our database"""
       for repo_data in self.enumerate_org_repositories(org):
           repo_fullname = f"{org}/{repo_data['name']}"

           # Skip if already known
           if repo_fullname in known_repos:
               continue

           # Get default branch
           default_branch = repo_data.get('default_branch', 'main')

           # Check for .datalad/config
           if self.check_datalad_config(org, repo_data['name'], default_branch):
               yield SearchResult(
                   id=repo_data['id'],
                   url=repo_data['html_url'],
                   name=repo_fullname,
                   dataset=True,
                   run=False,  # Can't detect RUNCMD without full search
                   container_run=False
               )
   ```

### Implementation Plan

#### Phase 1: Update `search_dataset_repos_in_org` Method
```python
def search_dataset_repos_in_org(self, org: str, known_repos: list = None) -> Iterator[SearchHit]:
    """Search for datasets within specific organization with fallback"""
    query = f"org:{org} path:.datalad filename:config"
    log.info(f"Searching for .datalad datasets in {org}")

    search_results = list(self.search("code", query))

    # Check if we need enumeration fallback
    if known_repos and self.needs_enumeration_fallback(org, search_results, known_repos):
        log.warning(f"Search returned empty for {org} despite known repos, using enumeration fallback")

        # Get known repo names for this org
        known_names = {r['name'] for r in known_repos
                      if r['name'].startswith(f"{org}/") and r.get('status') != 'gone'}

        # Enumerate and check unknown repositories
        for result in self.process_enumerated_repos(org, known_names):
            search_results.append(result)

    for hit in search_results:
        if isinstance(hit, SearchResult):
            yield hit  # From enumeration
        else:
            repo = SearchHit.from_repository(hit["repository"])
            log.info("Found %s", repo.name)
            yield repo
```

#### Phase 2: Pass Known Repositories to Searcher
```python
class GitHubUpdater:
    def get_new_collection(self, searcher: GitHubSearcher) -> list[GitHubRepo]:
        # Pass known repos to searcher for fallback detection
        known_repos = [{'name': r.name, 'status': r.status.value}
                      for r in self.all_repos.values()]

        search_results = searcher.get_datalad_repos(
            excluded_orgs=self.excluded_orgs,
            known_repos=known_repos  # NEW: Pass known repos
        )
        # ... rest of implementation
```

### Performance Considerations

1. **API Rate Limits**:
   - Repository enumeration: ~5000 requests/hour
   - Raw content checks: Not counted against API rate limit
   - Implement aggressive caching for enumeration results

2. **Optimization Strategies**:
   - Only use fallback for organizations with known empty search results
   - Cache repository lists for 24 hours
   - Batch raw content checks with parallel requests
   - Skip enumeration if organization has <50 known repos (search likely works)

3. **Gradual Rollout**:
   - Start with known problematic organizations (e.g., "dandisets")
   - Monitor API usage and adjust thresholds
   - Add organizations to fallback list as issues are discovered

### Expected Impact

- **dandisets organization**: Discover potentially 50+ new repositories currently missed
- **Improved coverage**: Catch repositories in organizations with search indexing issues
- **Future-proof**: Handles GitHub search degradation or policy changes

## Future Enhancements

1. **Dynamic thresholds**: Adjust exclusion threshold based on total repository count
2. **Parallel organization searches**: Search multiple orgs concurrently (within rate limits)
3. **Smart fallback detection**: Machine learning to predict when search will fail
4. **Incremental updates**: Track per-organization last-updated timestamps for more efficient re-runs
5. **RUNCMD detection in enumerated repos**: Parse recent commits for repositories found via enumeration
